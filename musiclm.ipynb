{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huang-Yongzhi/musiclm-pytorch/blob/main/musiclm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果重新运行，需要重启，不然trainer会报错，认为有多个实例"
      ],
      "metadata": {
        "id": "8rB8R87lnjHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 一、加载数据\n",
        "## 1.安装必要的包"
      ],
      "metadata": {
        "id": "DkjAlaMGnjHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install you-get\n",
        "!pip install yt-dlp\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AubllYUfS2mY",
        "outputId": "f005e142-f719-4780-cea7-0899910fe45b",
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: you-get in /usr/local/lib/python3.10/dist-packages (0.4.1650)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.10/dist-packages (2023.10.13)\n",
            "Requirement already satisfied: mutagen in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (1.47.0)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (3.19.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (1.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.加载数据集\n",
        "\n",
        "**数据集内容**\n",
        "\n",
        "调用的.csv文件内容如下\n",
        "```\n",
        "ytid,start_s,end_s,audioset_positive_labels,aspect_list,caption,author_id,is_balanced_subset,is_audioset_eval\n",
        "-0Gj8-vB1q4,30,40,\"/m/0140xf,/m/02cjck,/m/04rlf\",\"['low quality', 'sustained strings melody', 'soft female vocal', 'mellow piano melody', 'sad', 'soulful', 'ballad']\",\"The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.\",4,False,True\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "8tuFD0_Z-1AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  GitHub 上的 .csv 文件的 Raw 链接\n",
        "url = \"https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/c91b9f96775751aefa6f507fb304e7fd12182bf8/Data/musiccaps-public.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# 显示数据以验证加载正确\n",
        "print(df.head())\n",
        "\n",
        "# 指定文件保存路径\n",
        "file_path = '/content/musiccaps-public.csv'\n",
        "\n",
        "# 将 DataFrame 保存为 CSV 文件\n",
        "df.to_csv(file_path, index=False)  # 设置index=False以防止添加额外的索引列\n",
        "\n",
        "\n",
        "# 检查文件是否已保存\n",
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmLpHSV-ibtk",
        "outputId": "c49f7f06-6452-4fd3-a786-bff34eb41386"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ytid  start_s  end_s  \\\n",
            "0  -0Gj8-vB1q4       30     40   \n",
            "1  -0SdAVK79lg       30     40   \n",
            "2  -0vPFx-wRRI       30     40   \n",
            "3  -0xzrMun0Rs       30     40   \n",
            "4  -1LrH01Ei1w       30     40   \n",
            "\n",
            "                            audioset_positive_labels  \\\n",
            "0                       /m/0140xf,/m/02cjck,/m/04rlf   \n",
            "1  /m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...   \n",
            "2                                /m/025_jnm,/m/04rlf   \n",
            "3                                 /m/01g90h,/m/04rlf   \n",
            "4                                /m/02p0sh1,/m/04rlf   \n",
            "\n",
            "                                         aspect_list  \\\n",
            "0  ['low quality', 'sustained strings melody', 's...   \n",
            "1  ['guitar song', 'piano backing', 'simple percu...   \n",
            "2  ['amateur recording', 'finger snipping', 'male...   \n",
            "3  ['backing track', 'jazzy', 'digital drums', 'p...   \n",
            "4  ['rubab instrument', 'repetitive melody on dif...   \n",
            "\n",
            "                                             caption  author_id  \\\n",
            "0  The low quality recording features a ballad so...          4   \n",
            "1  This song features an electric guitar as the m...          0   \n",
            "2  a male voice is singing a melody with changing...          6   \n",
            "3  This song contains digital drums playing a sim...          6   \n",
            "4  This song features a rubber instrument being p...          0   \n",
            "\n",
            "   is_balanced_subset  is_audioset_eval  \n",
            "0               False              True  \n",
            "1               False             False  \n",
            "2               False              True  \n",
            "3               False              True  \n",
            "4               False             False  \n",
            "downloaded_audios\t\tresults\t\t\t     train_fine_transformer.py\n",
            "hubert_base_ls960_L9_km500.bin\tsample_data\t\t     train_semantic_transformer.py\n",
            "hubert_base_ls960.pt\t\tsemantic_transformer.pth\n",
            "musiccaps-public.csv\t\ttrain_coarse_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解释**：\n",
        "数据集是一个包含音频信息和描述的元数据文件，格式类似于 CSV。每行包含一个 YouTube 音频的标识符（ytid），音频的开始和结束时间（start_s 和 end_s），音频标签（audioset_positive_labels）和其他相关信息。"
      ],
      "metadata": {
        "id": "cmEZ4Rk2-8g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用如 youtube-dl 这类工具来下载视频，然后使用音频处理库（例如 librosa 或 pydub）来裁剪音频。以下是一个大致的步骤指南：\n",
        "\n",
        "## 使用 yt-dlp\n",
        "下载 YouTube 音频\n",
        "首先，您需要安装 yt-dlp。在 Colab 中，可以使用以下命令安装："
      ],
      "metadata": {
        "id": "puuachnT_ymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yt-dlp"
      ],
      "metadata": {
        "trusted": true,
        "id": "3ml1_hyMnjHJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试yt-dlp"
      ],
      "metadata": {
        "id": "gOVXSn4hnjHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !yt-dlp -f140 -x --audio-format mp3 https://www.youtube.com/watch?v=-0vPFx-wRRI"
      ],
      "metadata": {
        "trusted": true,
        "id": "KQi_rzeOnjHK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import glob\n",
        "\n",
        "ytid = \"-0xzrMun0Rs\"  # 示例 YouTube 视频ID\n",
        "start_s = 10  # 裁剪开始时间（秒）\n",
        "end_s = 20    # 裁剪结束时间（秒）\n",
        "audio_output_dir = '/content/downloaded_audios'  # 音频输出目录\n",
        "\n",
        "# 确保输出目录存在\n",
        "os.makedirs(audio_output_dir, exist_ok=True)\n",
        "\n",
        "video_url = f'https://www.youtube.com/watch?v={ytid}'\n",
        "temp_audio_path_pattern  = os.path.join(audio_output_dir, f'{ytid}_temp.*')\n",
        "output_audio_path = os.path.join(audio_output_dir, f'{ytid}.wav')\n",
        "\n",
        "\n",
        "try:\n",
        "    # 下载音频\n",
        "    subprocess.run(['yt-dlp', '-x', '--audio-format', 'wav', '-o', temp_audio_path_pattern, video_url], check=True)\n",
        "\n",
        "    # 查找下载的音频文件\n",
        "    downloaded_files = glob.glob(temp_audio_path_pattern)\n",
        "    if not downloaded_files:\n",
        "        raise Exception(\"Downloaded audio file not found.\")\n",
        "    downloaded_audio_path = downloaded_files[0]  # 取得实际下载的文件路径\n",
        "\n",
        "    # 使用 ffmpeg 裁剪音频\n",
        "    subprocess.run(['ffmpeg', '-i', downloaded_audio_path, '-ss', str(start_s), '-to', str(end_s), '-c', 'copy', output_audio_path], check=True)\n",
        "    os.remove(downloaded_audio_path)  # 删除临时文件\n",
        "    print(f\"Audio downloaded and trimmed: {output_audio_path}\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4eXawrdr2N_",
        "outputId": "5f7a723b-84a0-49bb-fadc-1a5e7b22f2a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['ffmpeg', '-i', '/content/downloaded_audios/-0xzrMun0Rs_temp.*.wav', '-ss', '10', '-to', '20', '-c', 'copy', '/content/downloaded_audios/-0xzrMun0Rs.wav']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 yt-dlp 命令中使用了 --postprocessor-args 选项，但没有指定具体哪个后处理器（post-processor）应该使用这些参数。\n",
        "我们需要参数应用于 ffmpeg 音频裁剪处理器。"
      ],
      "metadata": {
        "id": "ruD13Ii2njHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def download_audio(ytid, audio_output_dir):\n",
        "    video_url = f'https://www.youtube.com/watch?v={ytid}'\n",
        "    temp_audio_path_pattern  = os.path.join(audio_output_dir, f'{ytid}_temp.*')\n",
        "    output_audio_path = os.path.join(audio_output_dir, f'{ytid}.wav')\n",
        "\n",
        "    try:\n",
        "        # 下载音频\n",
        "        subprocess.run(['yt-dlp', '-x', '--audio-format', 'wav', '-o', temp_audio_path_pattern, video_url], check=True)\n",
        "        # 查找下载的音频文件\n",
        "        downloaded_files = glob.glob(temp_audio_path_pattern)\n",
        "        if not downloaded_files:\n",
        "            raise Exception(\"Downloaded audio file not found.\")\n",
        "        return downloaded_files[0], output_audio_path  # 返回实际下载的文件路径\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error downloading audio for {ytid}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def trim_audio(temp_audio_path, output_audio_path, start_s, end_s):\n",
        "    if not os.path.exists(temp_audio_path):\n",
        "        raise FileNotFoundError(f\"File not found: {temp_audio_path}\")\n",
        "\n",
        "    try:\n",
        "        # 使用 ffmpeg 裁剪音频\n",
        "        subprocess.run(['ffmpeg', '-i', temp_audio_path, '-ss', str(start_s), '-to', str(end_s), '-c', 'copy', output_audio_path], check=True)\n",
        "        os.remove(temp_audio_path)  # 删除临时文件\n",
        "        print(f\"Trimmed audio to {output_audio_path}.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error trimming audio: {e}\\nOutput: {e.stdout.decode()}\\nError: {e.stderr.decode()}\")\n",
        "\n",
        "def download_and_trim_audio(ytid, start_s, end_s, audio_output_dir):\n",
        "    temp_audio_path, output_audio_path = download_audio(ytid, audio_output_dir)\n",
        "    if temp_audio_path and output_audio_path:\n",
        "        trim_audio(temp_audio_path, output_audio_path, start_s, end_s)\n",
        "\n",
        "# 加载CSV文件\n",
        "csv_file = '/content/musiccaps-public.csv'  # colab\n",
        "# csv_file = '/kaggle/input/musiccaps/musiccaps-public.csv'  # kaggle\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "audio_output_dir = './downloaded_audios'  # 音频输出目录\n",
        "\n",
        "# 确保输出目录存在\n",
        "os.makedirs(audio_output_dir, exist_ok=True)\n",
        "\n",
        "# 使用线程池\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    for index, row in df.iterrows():\n",
        "        if index >= 20:\n",
        "            break\n",
        "        executor.submit(download_and_trim_audio, row['ytid'], row['start_s'], row['end_s'], audio_output_dir)\n"
      ],
      "metadata": {
        "id": "_Y8siM-O7JwO",
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 使用Youtube-dl会报错,改用you-get\n",
        "### 但是you-get不能给出固定文件名，不适合多线程"
      ],
      "metadata": {
        "id": "tYaUThnGTFT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install you-get"
      ],
      "metadata": {
        "trusted": true,
        "id": "Iy6n_lzjnjHM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试一下You-Get"
      ],
      "metadata": {
        "id": "ORHd9U-HUwP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !you-get -i 'https://www.youtube.com/watch?v=jNQXAC9IVRw'"
      ],
      "metadata": {
        "id": "v_IEcasAUvN5",
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查可用格式：运行 you-get 命令带 -i 选项（用于信息查看模式），查看该视频支持的所有可用格式。这样可以帮助您了解是否有特定的音频格式可供下载。执行命令如下："
      ],
      "metadata": {
        "id": "eGA7WSGWci_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !you-get -i \"https://www.youtube.com/watch?v=-0Gj8-vB1q4\"\n"
      ],
      "metadata": {
        "id": "eOXMETCOcf3q",
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看命令行是否正确"
      ],
      "metadata": {
        "id": "ILkch60cnjHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !you-get --no-caption -o \"./downloaded_videos\" --itag=160 \"https://www.youtube.com/watch?v=-0Gj8-vB1q4\"\n"
      ],
      "metadata": {
        "id": "EO6sNXqPbKBg",
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  提取音频\n",
        "安装**ffmpeg** 或其他类似工具来从下载的视频文件中提取音频。"
      ],
      "metadata": {
        "id": "798NAPzIwoFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get install ffmpeg\n"
      ],
      "metadata": {
        "id": "TrOcfuZ8wjfi",
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用无损编码器\n",
        "编解码器是 aac，这是 .m4a 文件的典型编解码器。\n",
        "\n",
        "要生成 .wav 文件，应该使用无损的编解码器（例如，pcm_s16le）"
      ],
      "metadata": {
        "id": "n6oesHXfnjHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import subprocess\n",
        "# import librosa\n",
        "# import soundfile as sf\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import glob # 用于文件路径名的模式匹配\n",
        "# from datetime import datetime\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "# def get_latest_file_in_dir(directory):\n",
        "#     \"\"\" 获取指定目录中最新的文件 \"\"\"\n",
        "#     list_of_files = glob.glob(os.path.join(directory, '*'))\n",
        "#     if not list_of_files:  # 如果目录为空\n",
        "#         return None\n",
        "#     latest_file = max(list_of_files, key=os.path.getmtime)\n",
        "#     return latest_file\n",
        "\n",
        "\n",
        "\n",
        "# def download_lowest_resolution_video(ytid, video_output_dir):\n",
        "#     video_url = f'https://www.youtube.com/watch?v={ytid}'\n",
        "#     try:\n",
        "#       # 使用 you-get 下载分辨率最低的视频\n",
        "#         subprocess.run(['you-get', '--no-caption', '-o', video_output_dir, '--itag=160', video_url], check=True)\n",
        "#         print (f\"Download {ytid} video.\")\n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         print(f\"Error downloading video {ytid}: {e}\")\n",
        "#         return None\n",
        "#     # 查找下载的视频文件\n",
        "#     return get_latest_file_in_dir(video_output_dir)\n",
        "\n",
        "\n",
        "\n",
        "# def extract_audio_from_video(video_path, output_audio_path):\n",
        "#     try:\n",
        "#         result = subprocess.run(['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', output_audio_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "#         file_name = video_path.split('/')[-1]\n",
        "#         print (f\"Extract {file_name} video.\")\n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         print(f\"Error extracting audio from video {video_path}: {e}\\nOutput: {e.stdout.decode()}\\nError: {e.stderr.decode()}\")\n",
        "\n",
        "\n",
        "\n",
        "# def download_and_extract(ytid, video_output_dir, audio_output_dir):\n",
        "#     downloaded_video = download_lowest_resolution_video(ytid, video_output_dir) # 下载视频\n",
        "#     if downloaded_video:\n",
        "#         audio_path = os.path.join(audio_output_dir, f'{ytid}.wav')\n",
        "#         extract_audio_from_video(downloaded_video, audio_path)\n",
        "\n",
        "\n",
        "# # 加载CSV文件\n",
        "# csv_file = '/kaggle/input/musiccaps/musiccaps-public.csv' # musiccaps-public.csv\n",
        "# df = pd.read_csv(csv_file)\n",
        "\n",
        "# video_output_dir = './downloaded_videos' # .downloaded_videos\n",
        "# audio_output_dir = './downloaded_audios' # ./downloaded_audios\n",
        "\n",
        "# # 确保输出目录存在\n",
        "# os.makedirs(video_output_dir, exist_ok=True)\n",
        "# os.makedirs(audio_output_dir, exist_ok=True)\n",
        "\n",
        "# # 初始化 test_n\n",
        "# # test_n = 0\n",
        "\n",
        "# # 遍历CSV文件，下载视频并提取音频\n",
        "# # 使用线程池\n",
        "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     for index, row in df.iterrows():\n",
        "#         if index >= 20:\n",
        "#             break\n",
        "#         executor.submit(download_and_extract, row['ytid'], video_output_dir, audio_output_dir)\n",
        "# #         if test_n >= 20:\n",
        "# #             break\n",
        "# #         test_n += 1"
      ],
      "metadata": {
        "id": "7euPjP2nS6eo",
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 二、下载训练模型的文件内容\n",
        "\n",
        "hubert_base_ls960.pt 文件是一个预训练的模型权重文件，用于 **HuBERT （Hidden Unit BERT）模型**。HuBERT 是由Facebook AI 研究团队开发的一种**自监督学习的语音识别模型**。它是基于 BERT 架构的，专门针对语音处理任务进行了优化。"
      ],
      "metadata": {
        "id": "RGx97LWZP0l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # 检查请求是否成功\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# 设置文件的URL和你想要保存的文件名\n",
        "file_url = \"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\"\n",
        "file_name = \"hubert_base_ls960.pt\"\n",
        "\n",
        "# 下载文件\n",
        "download_file(file_url, file_name)\n",
        "\n",
        "# 设置文件的URL和你想要保存的文件名\n",
        "file_url = \"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960_L9_km500.bin\"\n",
        "file_name = \"hubert_base_ls960_L9_km500.bin\"\n",
        "\n",
        "# 下载文件\n",
        "download_file(file_url, file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "1zRjmvmDzOir",
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SemanticTransformerTrainer（这可能是一个音频处理或自然语言处理相关的训练器）"
      ],
      "metadata": {
        "id": "HsF5TTf0QoVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 不是我们要的音频链接\n",
        "# import requests\n",
        "\n",
        "# url = \"https://github.com/hsfzxjy/models.storage/releases/download/HRNet-OCR/hrnet_cs_8090_torch11.pth\"\n",
        "# response = requests.get(url)\n",
        "# response.raise_for_status()\n",
        "\n",
        "# file_name = url.split('/')[-1]\n",
        "\n",
        "# with open(file_name, 'wb') as f:\n",
        "#   f.write(response.content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oOozQ_E_yPc2",
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前面都是数据准备，现在才是模型相关的。安装MusicLM必要的包"
      ],
      "metadata": {
        "id": "xtGEfkAdnjHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 重启也要运行\n",
        "!pip install musiclm-pytorch\n",
        "!pip install --upgrade tensorflow tensorflow-io\n",
        "!pip install audiolm_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSVTEQPWu5qB",
        "outputId": "abfcee3e-6f0b-4641-d56c-928dcea85f60",
        "execution": {
          "iopub.status.busy": "2023-11-13T07:17:24.120848Z",
          "iopub.execute_input": "2023-11-13T07:17:24.121247Z",
          "iopub.status.idle": "2023-11-13T07:18:01.355871Z",
          "shell.execute_reply.started": "2023-11-13T07:17:24.121212Z",
          "shell.execute_reply": "2023-11-13T07:18:01.354639Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: musiclm-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (0.24.1)\n",
            "Requirement already satisfied: audiolm-pytorch>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (1.7.6)\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (0.16.4)\n",
            "Requirement already satisfied: einops>=0.6 in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (0.7.0)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (0.1.2)\n",
            "Requirement already satisfied: vector-quantize-pytorch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (1.11.7)\n",
            "Requirement already satisfied: x-clip in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (0.14.4)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from musiclm-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: ema-pytorch>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.0)\n",
            "Requirement already satisfied: encodec in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.1)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.12.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.2)\n",
            "Requirement already satisfied: local-attention>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.35.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->musiclm-pytorch) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->musiclm-pytorch) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->musiclm-pytorch) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->musiclm-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->musiclm-pytorch) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->musiclm-pytorch) (2.1.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from x-clip->musiclm-pytorch) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from x-clip->musiclm-pytorch) (2023.6.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from x-clip->musiclm-pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.0.5)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.0.6)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.3.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate->musiclm-pytorch) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->musiclm-pytorch) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.11.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12->musiclm-pytorch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->x-clip->musiclm-pytorch) (9.4.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.9.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (2023.7.22)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: audiolm_pytorch in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Requirement already satisfied: accelerate>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.24.1)\n",
            "Requirement already satisfied: beartype>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.16.4)\n",
            "Requirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.7.0)\n",
            "Requirement already satisfied: ema-pytorch>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.3.0)\n",
            "Requirement already satisfied: encodec in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.1.1)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.12.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (1.3.2)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.1.2)\n",
            "Requirement already satisfied: local-attention>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (1.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (4.35.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (4.66.1)\n",
            "Requirement already satisfied: vector-quantize-pytorch>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from audiolm_pytorch) (1.11.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm_pytorch) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm_pytorch) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm_pytorch) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm_pytorch) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->audiolm_pytorch) (2.1.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (3.0.5)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (2.3.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq->audiolm_pytorch) (2.8.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->audiolm_pytorch) (1.11.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->audiolm_pytorch) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->audiolm_pytorch) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->audiolm_pytorch) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->audiolm_pytorch) (0.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm_pytorch) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (4.9.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->audiolm_pytorch) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->audiolm_pytorch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->audiolm_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->audiolm_pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->audiolm_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->audiolm_pytorch) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12->audiolm_pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage\n",
        "`MuLaN` first needs to be trained"
      ],
      "metadata": {
        "id": "tHqjcLn9RZwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 重启也要运行\n",
        "\n",
        "import array\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy\n",
        "import soundfile\n",
        "from scipy.io.wavfile import read\n",
        "import torch\n",
        "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
        "import os\n",
        "import pathlib\n",
        "import numpy\n",
        "import pandas\n",
        "from musiclm_pytorch import MuLaNEmbedQuantizer\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T07:18:01.358048Z",
          "iopub.execute_input": "2023-11-13T07:18:01.358377Z",
          "iopub.status.idle": "2023-11-13T07:18:35.735271Z",
          "shell.execute_reply.started": "2023-11-13T07:18:01.358348Z",
          "shell.execute_reply": "2023-11-13T07:18:35.734213Z"
        },
        "trusted": true,
        "id": "EFiBKvASnjHT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 验证环境\n",
        "如果没有CUDA环境，就要将后面的.cuda()去掉"
      ],
      "metadata": {
        "id": "0T1MJ_TMnjHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T07:18:35.736413Z",
          "iopub.execute_input": "2023-11-13T07:18:35.737026Z",
          "iopub.status.idle": "2023-11-13T07:18:35.768806Z",
          "shell.execute_reply.started": "2023-11-13T07:18:35.736995Z",
          "shell.execute_reply": "2023-11-13T07:18:35.767742Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsKBSiTnnjHU",
        "outputId": "fd5d45fa-7f8f-4b0a-ae96-c98e0702316a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 加载三个transformer\n",
        "`**MusicLM**` 用了三个transformer，`MuLan(Audio)`, `w2v-BERT`, `SoundStream`\n",
        "![image.png](attachment:b35cb9db-0628-477f-af00-cf4b6437741d.png)\n",
        "\n",
        "**声学模块SoundStream**：端到端神经音频编解码器，能提供更高质量的音频，并扩展至编码不同的声音类型\n",
        "\n",
        "**语义模块w2v-BERT**：使用该模型的掩码语言建模(MLM)模块的中间层。在预训练和冻结模型之后，从第7层提取embedding，并使用学习到的k-means质心对embedding进行量化。该模块主要起到提取语义词元的作用\n",
        "\n",
        "**音频文本对MuLan**：采用双塔并行编码器架构，使用对比损失进行训练，在音乐音频和文本之间形成共享嵌入空间。即`音频信号和文本语义都具有基于离散标记的同质表示`\n"
      ],
      "metadata": {
        "id": "hfyNTk9pnjHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 重启也要运行\n",
        "import torch\n",
        "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
        "\n",
        "audio_transformer = AudioSpectrogramTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64,\n",
        "    spec_n_fft = 128,\n",
        "    spec_win_length = 24,\n",
        "    spec_aug_stretch_factor = 0.8\n",
        ")\n",
        "\n",
        "text_transformer = TextTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64\n",
        ")\n",
        "\n",
        "mulan = MuLaN(\n",
        "    audio_transformer = audio_transformer,\n",
        "    text_transformer = text_transformer\n",
        ")\n",
        "\n",
        "# get a ton of <sound, text> pairs and train\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "texts = torch.randint(0, 20000, (2, 256))\n",
        "\n",
        "loss = mulan(wavs, texts)\n",
        "loss.backward()\n",
        "\n",
        "# after much training, you can embed sounds and text into a joint embedding space\n",
        "# for conditioning the audio LM\n",
        "\n",
        "embeds = mulan.get_audio_latents(wavs)  # during training\n",
        "\n",
        "embeds = mulan.get_text_latents(texts)  # during inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24y5bccQu9XA",
        "outputId": "3cf21cae-00fd-4b7a-a46b-eb5354388d3b",
        "execution": {
          "iopub.status.busy": "2023-11-13T07:18:35.770344Z",
          "iopub.execute_input": "2023-11-13T07:18:35.770774Z",
          "iopub.status.idle": "2023-11-13T07:18:37.131865Z",
          "shell.execute_reply.started": "2023-11-13T07:18:35.770733Z",
          "shell.execute_reply": "2023-11-13T07:18:37.130852Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spectrogram yielded shape of (65, 86), but had to be cropped to (64, 80) to be patchified for transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the conditioning embeddings for the three transformers that are a part of AudioLM, you must use the `MuLaNEmbedQuantizer` as so"
      ],
      "metadata": {
        "id": "7lzG0rfhRgia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MusicLM"
      ],
      "metadata": {
        "id": "6zcvphTvnjHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 重启也要运行\n",
        "from musiclm_pytorch import MuLaNEmbedQuantizer\n",
        "\n",
        "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
        "\n",
        "quantizer = MuLaNEmbedQuantizer(\n",
        "    mulan = mulan,                          # pass in trained mulan from above\n",
        "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
        "    namespaces = ('semantic', 'coarse', 'fine')\n",
        ")\n",
        "\n",
        "# now say you want the conditioning embeddings for semantic transformer\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers"
      ],
      "metadata": {
        "id": "1mPZ82TAvFDQ",
        "execution": {
          "iopub.status.busy": "2023-11-13T07:18:37.134020Z",
          "iopub.execute_input": "2023-11-13T07:18:37.134330Z",
          "iopub.status.idle": "2023-11-13T07:18:37.649699Z",
          "shell.execute_reply.started": "2023-11-13T07:18:37.134302Z",
          "shell.execute_reply": "2023-11-13T07:18:37.648809Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.train Hubert模型，audioLM transformer training, train AudioLM,\n",
        "To train (or finetune) the three transformers that are a part of `AudioLM`, you simply follow the instructions over at `audiolm-pytorch` for training, but pass in the `MulanEmbedQuantizer` instance to the training classes under the keyword `audio_conditioner`\n",
        "\n",
        "ex. `SemanticTransformerTrainer`"
      ],
      "metadata": {
        "id": "yk7RkyYnRrwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "验证路径是否正确"
      ],
      "metadata": {
        "id": "RplqLBIanjHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Files in './downloaded_audios':\", os.listdir('./downloaded_audios'))\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqNdBNuxnjHX",
        "outputId": "fe7afee2-7dad-4110-9427-18f17bcafde3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Files in './downloaded_audios': ['-5f6hjZf9Yw.wav', '-7B9tPuIP-w.wav', '-0xzrMun0Rs_temp.*.wav', '-1UWSisR2zo_temp.*.wav', '-88me9bBzrk_temp.*.wav', '-5f6hjZf9Yw_temp.*.wav', '-5FoeegAgvU_temp.*.wav', '-0vPFx-wRRI_temp.*.wav', '-1LrH01Ei1w.wav', '-0SdAVK79lg.wav', '-7wUQP6G5EQ.wav', '-6HBGg1cAI0_temp.*.wav', '-4NLarMj4xU.wav', '-5xOcMJpTUk.wav', '-6pcgdLfb_A.wav', '-0SdAVK79lg_temp.*.wav', '-7B9tPuIP-w_temp.*.wav', '-6QGvxvaTkI_temp.*.wav', '-7wUQP6G5EQ_temp.*.wav', '-0xzrMun0Rs.wav', '-6pcgdLfb_A_temp.*.wav', '-0Gj8-vB1q4_temp.*.wav', '-5xOcMJpTUk_temp.*.wav', '-8C-gydUbR8.wav', '-5FoeegAgvU.wav', '-4SYC2YgzL8_temp.*.wav', '-6HBGg1cAI0.wav', '-1OlgJWehn8.wav', '-1UWSisR2zo.wav', '-1LrH01Ei1w_temp.*.wav', '-88me9bBzrk.wav', '-3Kv4fdm7Uk_temp.*.wav', '-4SYC2YgzL8.wav', '-6QGvxvaTkI.wav', '-4NLarMj4xU_temp.*.wav', '-0vPFx-wRRI.wav', '-1OlgJWehn8_temp.*.wav', '-8C-gydUbR8_temp.*.wav', '-3Kv4fdm7Uk.wav', '-0Gj8-vB1q4.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "函数封装：将创建和使用训练器的代码封装在一个函数中。这有助于在函数结束时自动销毁局部变量，包括训练器实例。但这只在训练器没有修改全局状态或在内部维持静态/全局变量的情况下有效。\n",
        "或者每次注释掉一个transformer，然后重启运行\n",
        "\n",
        "防止触发\n",
        "\n",
        "AssertionError: only one Trainer can be instantiated at a time for training\n",
        "\n"
      ],
      "metadata": {
        "id": "UEeqmyGSnjHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多线程解决one trainer问题"
      ],
      "metadata": {
        "id": "83uGNsQenjHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成`train_semantic_transformer.py` 防止trainer冲突"
      ],
      "metadata": {
        "id": "DFd1Vh8e7ALZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_semantic_transformer.py\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans\n",
        "# from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer\n",
        "# from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "# from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "# from audiolm_pytorch import AudioLMSoundStream, AudioLM\n",
        "# import gc  # 导入垃圾回收模块\n",
        "\n",
        "# # 公共变量\n",
        "# checkpoint_path = 'hubert_base_ls960.pt'\n",
        "# kmeans_path = 'hubert_base_ls960_L9_km500.bin'\n",
        "\n",
        "# audio_output_dir = './downloaded_audios'\n",
        "# batch_size = 1\n",
        "# data_max_length = 320 * 32\n",
        "# num_train_steps = 1\n",
        "\n",
        "# # 函数：训练 SemanticTransformer\n",
        "# def train_semantic_transformer():\n",
        "#     wav2vec = HubertWithKmeans(checkpoint_path=checkpoint_path, kmeans_path=kmeans_path)   # 每个函数中重新创建 wav2vec，后面会删掉\n",
        "#     soundstream = AudioLMSoundStream()\n",
        "#     semantic_transformer = SemanticTransformer(num_semantic_tokens=wav2vec.codebook_size, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "#     trainer = SemanticTransformerTrainer(transformer=semantic_transformer, wav2vec=wav2vec, audio_conditioner=quantizer, folder=audio_output_dir, batch_size=batch_size, data_max_length=data_max_length, num_train_steps=num_train_steps)\n",
        "#     trainer.train()\n",
        "#     torch.save(semantic_transformer.state_dict(), 'semantic_transformer.pth')\n",
        "#     del semantic_transformer, trainer, wav2vec\n",
        "#     gc.collect()  # 执行垃圾回收\n",
        "\n",
        "\n",
        "\n",
        "# # 依次训练每个模型\n",
        "# train_semantic_transformer()\n"
      ],
      "metadata": {
        "id": "uH0MqXnSpW33"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train_coarse_transformer.py`"
      ],
      "metadata": {
        "id": "80RSbKlt7WMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_coarse_transformer.py\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans\n",
        "# from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer\n",
        "# from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "# from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "# from audiolm_pytorch import AudioLMSoundStream, AudioLM\n",
        "# import gc  # 导入垃圾回收模块\n",
        "\n",
        "# # 公共变量\n",
        "# checkpoint_path = 'hubert_base_ls960.pt'\n",
        "# kmeans_path = 'hubert_base_ls960_L9_km500.bin'\n",
        "\n",
        "# audio_output_dir = './downloaded_audios'\n",
        "# batch_size = 1\n",
        "# data_max_length = 320 * 32\n",
        "# num_train_steps = 1\n",
        "\n",
        "# # 函数：训练 CoarseTransformer\n",
        "# def train_coarse_transformer():\n",
        "#     wav2vec = HubertWithKmeans(checkpoint_path=checkpoint_path, kmeans_path=kmeans_path)   # 每个函数中重新创建 wav2vec，后面会删掉\n",
        "#     soundstream = AudioLMSoundStream()\n",
        "\n",
        "#     coarse_transformer = CoarseTransformer(num_semantic_tokens=wav2vec.codebook_size, codebook_size=1024, num_coarse_quantizers=4, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "#     trainer = CoarseTransformerTrainer(transformer=coarse_transformer, codec=soundstream, wav2vec=wav2vec, audio_conditioner=quantizer, folder=audio_output_dir, batch_size=batch_size, data_max_length=data_max_length, num_train_steps=num_train_steps)\n",
        "#     trainer.train()\n",
        "#     torch.save(coarse_transformer.state_dict(), 'coarse_transformer.pth')\n",
        "#     del coarse_transformer, trainer, wav2vec, soundstream\n",
        "#     gc.collect()\n",
        "\n",
        "# train_coarse_transformer()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "baXcz5lC3Jyx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train_fine_transformer.py`"
      ],
      "metadata": {
        "id": "0H-4ZhEU7o7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_fine_transformer.py\n",
        "\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans\n",
        "# from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer\n",
        "# from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "# from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "# from audiolm_pytorch import AudioLMSoundStream, AudioLM\n",
        "# import gc  # 导入垃圾回收模块\n",
        "\n",
        "# # 公共变量\n",
        "# checkpoint_path = 'hubert_base_ls960.pt'\n",
        "# kmeans_path = 'hubert_base_ls960_L9_km500.bin'\n",
        "\n",
        "# audio_output_dir = './downloaded_audios'\n",
        "# batch_size = 1\n",
        "# data_max_length = 320 * 32\n",
        "# num_train_steps = 1\n",
        "\n",
        "\n",
        "# # 函数：训练 FineTransformer\n",
        "# def train_fine_transformer():\n",
        "#     soundstream = AudioLMSoundStream()\n",
        "\n",
        "#     fine_transformer = FineTransformer(num_coarse_quantizers=4, num_fine_quantizers=8, codebook_size=1024, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "#     trainer = FineTransformerTrainer(transformer=fine_transformer, codec=soundstream, folder=audio_output_dir, batch_size=batch_size, data_max_length=data_max_length, num_train_steps=num_train_steps)\n",
        "#     trainer.train()\n",
        "#     torch.save(fine_transformer.state_dict(), 'fine_transformer.pth')\n",
        "#     del fine_transformer, trainer, soundstream\n",
        "#     gc.collect()\n",
        "\n",
        "# train_fine_transformer()"
      ],
      "metadata": {
        "id": "uQG_GAub3Qxg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "强制覆盖文件"
      ],
      "metadata": {
        "id": "e6Zt92_39_gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O train_semantic_transformer.py https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_semantic_transformer.py\n",
        "!wget -O train_coarse_transformer.py https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_coarse_transformer.py\n",
        "!wget -O train_fine_transformer.py https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_fine_transformer.py\n",
        "\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n",
        "%run train_semantic_transformer.py\n",
        "\n",
        "\n",
        "# !python train_semantic_transformer.py\n",
        "# !python train_coarse_transformer.py\n",
        "# !python train_fine_transformer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnMT1H10_XTq",
        "outputId": "7b01c3ea-438e-4ff7-ed23-2c137b5d4c6e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-13 14:49:51--  https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_semantic_transformer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2623 (2.6K) [text/plain]\n",
            "Saving to: ‘train_semantic_transformer.py’\n",
            "\n",
            "train_semantic_tran 100%[===================>]   2.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-13 14:49:51 (42.3 MB/s) - ‘train_semantic_transformer.py’ saved [2623/2623]\n",
            "\n",
            "--2023-11-13 14:49:51--  https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_coarse_transformer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2631 (2.6K) [text/plain]\n",
            "Saving to: ‘train_coarse_transformer.py’\n",
            "\n",
            "train_coarse_transf 100%[===================>]   2.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-13 14:49:51 (43.6 MB/s) - ‘train_coarse_transformer.py’ saved [2631/2631]\n",
            "\n",
            "--2023-11-13 14:49:51--  https://raw.githubusercontent.com/Huang-Yongzhi/musiclm-pytorch/main/train_fine_transformer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264 (1.2K) [text/plain]\n",
            "Saving to: ‘train_fine_transformer.py’\n",
            "\n",
            "train_fine_transfor 100%[===================>]   1.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-13 14:49:52 (112 MB/s) - ‘train_fine_transformer.py’ saved [1264/1264]\n",
            "\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "training with dataset of 38 samples and validating with randomly splitted 2 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) y\n",
            "0: loss: 6.254929542541504\n",
            "0: valid loss 6.357746601104736\n",
            "0: saving model to results\n",
            "training complete\n",
            "save semantic_transformer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train_coarse_transformer.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "cIJ8rUVZJe08",
        "outputId": "50d0b438-0d92-4456-c47f-29ef482e403b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/content/train_coarse_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrain_coarse_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/train_coarse_transformer.py\u001b[0m in \u001b[0;36mtrain_coarse_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcoarse_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoarseTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_semantic_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwav2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodebook_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_coarse_quantizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_text_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoarseTransformerTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoarse_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoundstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwav2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_conditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoarse_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coarse_transformer.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(audiolm_pytorch.trainer.CoarseTransformerTrainer.__init__) at 0x7a4b12b86d40>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_101767820511440, __beartype_object_134462867528320, __beartype_object_134462854811264, __beartype_object_134462884273600, __beartype_object_101767605177152, __beartype_object_101767723768208, __beartype_getrandbits, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transformer, codec, wav2vec, num_train_steps, batch_size, audio_conditioner, dataset, valid_dataset, ds_fields, data_max_length, data_max_length_seconds, folder, lr, grad_accum_every, wd, max_grad_norm, valid_frac, random_split_seed, save_results_every, save_model_every, results_folder, accelerate_kwargs, init_process_group_timeout_seconds, split_batches, drop_last, force_clear_prev_results, average_valid_loss_over_grad_accum_every)\u001b[0m\n\u001b[1;32m    915\u001b[0m     ):\n\u001b[1;32m    916\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mcheck_one_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0minit_process_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInitProcessGroupKwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_process_group_timeout_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/trainer.py\u001b[0m in \u001b[0;36mcheck_one_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_one_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'only one Trainer can be instantiated at a time for training'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: only one Trainer can be instantiated at a time for training"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train_fine_transformer.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "2Vrcpip8JhOs",
        "outputId": "4852c93e-8f3b-4c95-ad1c-192582f90bab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/content/train_fine_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrain_fine_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/train_fine_transformer.py\u001b[0m in \u001b[0;36mtrain_fine_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfine_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFineTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_coarse_quantizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_fine_quantizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_text_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFineTransformerTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfine_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoundstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fine_transformer.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(audiolm_pytorch.trainer.FineTransformerTrainer.__init__) at 0x7a4b12b87490>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_101767820512400, __beartype_object_134462867528320, __beartype_object_134462884273600, __beartype_object_101767605177152, __beartype_object_101767723768208, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transformer, codec, num_train_steps, batch_size, audio_conditioner, dataset, valid_dataset, data_max_length, data_max_length_seconds, dataset_normalize, folder, lr, grad_accum_every, wd, max_grad_norm, valid_frac, random_split_seed, save_results_every, save_model_every, results_folder, accelerate_kwargs, init_process_group_timeout_seconds, split_batches, drop_last, force_clear_prev_results, average_valid_loss_over_grad_accum_every)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     ):\n\u001b[1;32m   1190\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mcheck_one_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0minit_process_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInitProcessGroupKwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_process_group_timeout_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/trainer.py\u001b[0m in \u001b[0;36mcheck_one_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_one_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'only one Trainer can be instantiated at a time for training'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mONE_TRAINER_INSTANTIATED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: only one Trainer can be instantiated at a time for training"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7Z8PqbDCWm",
        "outputId": "ad707266-b585-4b38-a540-6b205858965f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloaded_audios\t\tresults\t\t\t     train_fine_transformer.py\n",
            "hubert_base_ls960_L9_km500.bin\tsample_data\t\t     train_semantic_transformer.py\n",
            "hubert_base_ls960.pt\t\tsemantic_transformer.pth\n",
            "musiccaps-public.csv\t\ttrain_coarse_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from audiolm_pytorch import HubertWithKmeans\n",
        "from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer\n",
        "from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "from audiolm_pytorch import AudioLMSoundStream, AudioLM\n",
        "\n",
        "# 创建并加载 AudioLM 实例\n",
        "wav2vec = HubertWithKmeans(checkpoint_path=checkpoint_path, kmeans_path=kmeans_path) # 前面被删掉了\n",
        "soundstream = AudioLMSoundStream()\n",
        "\n",
        "semantic_transformer = SemanticTransformer(num_semantic_tokens=wav2vec.codebook_size, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "coarse_transformer = CoarseTransformer(num_semantic_tokens=wav2vec.codebook_size, codebook_size=1024, num_coarse_quantizers=4, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "fine_transformer = FineTransformer(num_coarse_quantizers=4, num_fine_quantizers=8, codebook_size=1024, dim=1024, depth=6, audio_text_condition=True).cuda()\n",
        "\n",
        "# 加载模型状态\n",
        "semantic_transformer.load_state_dict(torch.load('semantic_transformer.pth'))\n",
        "coarse_transformer.load_state_dict(torch.load('coarse_transformer.pth'))\n",
        "fine_transformer.load_state_dict(torch.load('fine_transformer.pth'))\n",
        "\n",
        "audiolm = AudioLM(wav2vec=wav2vec, codec=soundstream, semantic_transformer=semantic_transformer, coarse_transformer=coarse_transformer, fine_transformer=fine_transformer)\n"
      ],
      "metadata": {
        "id": "rA8dOWf53f9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 重启也要运行\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans\n",
        "# from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, SoundStream\n",
        "# from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "# from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "# from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n",
        "# from unittest import mock\n",
        "# import multiprocessing as mp\n",
        "# import gc  # 导入垃圾回收模块\n",
        "\n",
        "\n",
        "# # 设置多进程启动方式为 'spawn'\n",
        "# mp.set_start_method('spawn', force=True)\n",
        "\n",
        "# def train_semantic_transformer_process(checkpoint_path, kmeans_path):\n",
        "#     wav2vec = HubertWithKmeans(checkpoint_path=checkpoint_path, kmeans_path=kmeans_path)\n",
        "#     semantic_transformer = SemanticTransformer(\n",
        "#         num_semantic_tokens = wav2vec.codebook_size,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
        "#     ).cuda()\n",
        "\n",
        "#     trainer = SemanticTransformerTrainer(\n",
        "#         transformer = semantic_transformer,\n",
        "#         wav2vec = wav2vec,\n",
        "#         audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
        "#         folder ='./downloaded_audios',\n",
        "#         batch_size = 1,\n",
        "#         data_max_length = 320 * 32,\n",
        "#         num_train_steps = 1\n",
        "#     )\n",
        "\n",
        "#     trainer.train()\n",
        "#     torch.save(semantic_transformer.state_dict(), '/content/semantic_transformer.pth') # 保存模型\n",
        "# #     del trainer  # 显式删除实例，删除好像没有效果，需要存下来重启\n",
        "\n",
        "\n",
        "# def train_coarse_transformer_process(checkpoint_path, kmeans_path):\n",
        "#     wav2vec = HubertWithKmeans(checkpoint_path=checkpoint_path, kmeans_path=kmeans_path)\n",
        "#     soundstream = MusicLMSoundStream()\n",
        "\n",
        "#     coarse_transformer = CoarseTransformer(\n",
        "#         num_semantic_tokens = wav2vec.codebook_size,\n",
        "#         codebook_size = 1024,\n",
        "#         num_coarse_quantizers = 4,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True\n",
        "#     ).cuda()\n",
        "\n",
        "#     with mock.patch('builtins.input', return_value='n'):\n",
        "#         trainer = CoarseTransformerTrainer(\n",
        "#             transformer = coarse_transformer,\n",
        "#             codec = soundstream,\n",
        "#             wav2vec = wav2vec,\n",
        "#             audio_conditioner = quantizer,\n",
        "#             folder = './downloaded_audios/',\n",
        "#             batch_size = 1,\n",
        "#             data_max_length = 320 * 32,\n",
        "#             num_train_steps = 1\n",
        "#         )\n",
        "#         trainer.train()\n",
        "#         # 保存模型状态字典\n",
        "#         torch.save(coarse_transformer.state_dict(), '/content/coarse_transformer.pth') # 保存模型\n",
        "\n",
        "\n",
        "# def train_fine_transformer_process():\n",
        "#     soundstream = MusicLMSoundStream()\n",
        "#     fine_transformer = FineTransformer(\n",
        "#         num_coarse_quantizers = 4,\n",
        "#         num_fine_quantizers = 8,\n",
        "#         codebook_size = 1024,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True\n",
        "#     ).cuda()\n",
        "\n",
        "#     with mock.patch('builtins.input', return_value='n'):\n",
        "#         trainer = FineTransformerTrainer(\n",
        "#             transformer = fine_transformer,\n",
        "#             codec = soundstream,\n",
        "#             folder = './downloaded_audios/',\n",
        "#             batch_size = 1,\n",
        "#             data_max_length = 320 * 32,\n",
        "#             num_train_steps = 1,\n",
        "#             audio_conditioner = quantizer\n",
        "#         )\n",
        "\n",
        "#         trainer.train()\n",
        "#         torch.save(fine_transformer.state_dict(), '/content/fine_transformer.pth') # 保存模型\n",
        "\n",
        "\n",
        "\n",
        "# def run_training_process(train_function, *args):\n",
        "#     process = mp.Process(target=train_function, args=args)\n",
        "#     process.start()\n",
        "#     process.join()  # 等待进程结束\n",
        "\n",
        "\n",
        "\n",
        "# def train_all_transformers():\n",
        "\n",
        "#     # 加载 wav2vec 的参数\n",
        "#     checkpoint_path = 'hubert_base_ls960.pt'\n",
        "#     kmeans_path = 'hubert_base_ls960_L9_km500.bin'\n",
        "\n",
        "#     # 多线程运行\n",
        "#     run_training_process(train_semantic_transformer_process, checkpoint_path, kmeans_path)\n",
        "#     run_training_process(train_coarse_transformer_process, checkpoint_path, kmeans_path)\n",
        "#     run_training_process(train_fine_transformer_process)\n",
        "\n",
        "# train_all_transformers()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T07:27:18.164969Z",
          "iopub.execute_input": "2023-11-13T07:27:18.165867Z"
        },
        "trusted": true,
        "id": "b4r8SNwJnjHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面是单线程运行，但是需要每次调用一个网络训练。"
      ],
      "metadata": {
        "id": "LcJ7f-EMnjHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 重启也要运行\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans\n",
        "# from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, SoundStream\n",
        "# from audiolm_pytorch import CoarseTransformer, CoarseTransformerTrainer\n",
        "# from audiolm_pytorch import FineTransformer, FineTransformerTrainer\n",
        "# from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n",
        "# from unittest import mock\n",
        "# import multiprocessing\n",
        "\n",
        "\n",
        "# def train_semantic_transformer(wav2vec):\n",
        "\n",
        "#     semantic_transformer = SemanticTransformer(\n",
        "#         num_semantic_tokens = wav2vec.codebook_size,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
        "#     ).cuda()\n",
        "\n",
        "#     trainer = SemanticTransformerTrainer(\n",
        "#         transformer = semantic_transformer,\n",
        "#         wav2vec = wav2vec,\n",
        "#         audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
        "#         folder ='./downloaded_audios',\n",
        "#         batch_size = 1,\n",
        "#         data_max_length = 320 * 32,\n",
        "#         num_train_steps = 1\n",
        "#     )\n",
        "\n",
        "#     trainer.train()\n",
        "#     torch.save(semantic_transformer.state_dict(), 'semantic_transformer.pth') # 保存模型\n",
        "#     del trainer  # 显式删除实例，删除好像没有效果，需要存下来重启\n",
        "\n",
        "\n",
        "# def train_coarse_transformer(wav2vec, soundstream):\n",
        "\n",
        "#     coarse_transformer = CoarseTransformer(\n",
        "#         num_semantic_tokens = wav2vec.codebook_size,\n",
        "#         codebook_size = 1024,\n",
        "#         num_coarse_quantizers = 4,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True\n",
        "#     ).cuda()\n",
        "\n",
        "#     with mock.patch('builtins.input', return_value='n'):\n",
        "#         trainer = CoarseTransformerTrainer(\n",
        "#             transformer = coarse_transformer,\n",
        "#             codec = soundstream,\n",
        "#             wav2vec = wav2vec,\n",
        "#             audio_conditioner = quantizer,\n",
        "#             folder = './downloaded_audios/',\n",
        "#             batch_size = 1,\n",
        "#             data_max_length = 320 * 32,\n",
        "#             num_train_steps = 1\n",
        "#         )\n",
        "#         trainer.train()\n",
        "#         # 保存模型状态字典\n",
        "#         torch.save(coarse_transformer.state_dict(), 'coarse_transformer.pth') # 保存模型\n",
        "\n",
        "# def train_fine_transformer(soundstream):\n",
        "#     fine_transformer = FineTransformer(\n",
        "#         num_coarse_quantizers = 4,\n",
        "#         num_fine_quantizers = 8,\n",
        "#         codebook_size = 1024,\n",
        "#         dim = 1024,\n",
        "#         depth = 6,\n",
        "#         audio_text_condition = True\n",
        "#     ).cuda()\n",
        "\n",
        "#     with mock.patch('builtins.input', return_value='n'):\n",
        "#         trainer = FineTransformerTrainer(\n",
        "#             transformer = fine_transformer,\n",
        "#             codec = soundstream,\n",
        "#             folder = './downloaded_audios/',\n",
        "#             batch_size = 1,\n",
        "#             data_max_length = 320 * 32,\n",
        "#             num_train_steps = 1,\n",
        "#             audio_conditioner = quantizer\n",
        "#         )\n",
        "\n",
        "#         trainer.train()\n",
        "#         torch.save(fine_transformer.state_dict(), 'fine_transformer.pth') # 保存模型\n",
        "\n",
        "# def train_all_transformers():\n",
        "#      # soundstream = SoundStream.init_and_load_from('/path/to/trained/soundstream.pt')\n",
        "#     soundstream = MusicLMSoundStream()\n",
        "#     # 重新加载 wav2vec 和 soundstream\n",
        "#     wav2vec = HubertWithKmeans(\n",
        "#         checkpoint_path='hubert_base_ls960.pt',\n",
        "#         kmeans_path='hubert_base_ls960_L9_km500.bin'\n",
        "#     )\n",
        "\n",
        "#    # 训练 SemanticTransformer\n",
        "#     train_semantic_transformer(wav2vec)\n",
        "#     # 训练 CoarseTransformer\n",
        "#     train_coarse_transformer(wav2vec, soundstream)\n",
        "\n",
        "#     # 训练 FineTransformer\n",
        "#     train_fine_transformer(soundstream)\n",
        "\n",
        "# train_all_transformers()\n"
      ],
      "metadata": {
        "id": "ShVM60ELvHnh",
        "execution": {
          "iopub.status.busy": "2023-11-13T07:18:37.651345Z",
          "iopub.execute_input": "2023-11-13T07:18:37.651929Z",
          "iopub.status.idle": "2023-11-13T07:18:58.886553Z",
          "shell.execute_reply.started": "2023-11-13T07:18:37.651889Z",
          "shell.execute_reply": "2023-11-13T07:18:58.885143Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "调用训练模型"
      ],
      "metadata": {
        "id": "-mby2TpAnjHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_semantic_transformer()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2z8dR7--njHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **重启后**，重新创建 semantic_transformer 实例"
      ],
      "metadata": {
        "id": "2LeSdwL6njHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 重启也要运行\n",
        "# # 重启后导入必要的库\n",
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, AudioLM\n",
        "\n",
        "# # 重新创建和初始化模型\n",
        "# wav2vec = HubertWithKmeans(\n",
        "#     checkpoint_path='hubert_base_ls960.pt',\n",
        "#     kmeans_path='hubert_base_ls960_L9_km500.bin'\n",
        "# )\n",
        "# semantic_transformer = SemanticTransformer(\n",
        "#     num_semantic_tokens=wav2vec.codebook_size,\n",
        "#     dim=1024, depth=6,\n",
        "#     audio_text_condition=True).cuda()\n",
        "\n",
        "# # 加载之前保存的模型状态\n",
        "# semantic_transformer.load_state_dict(torch.load('semantic_transformer.pth'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ALvXCXEQnjHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_coarse_transformer()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OxGyV8aPnjHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **重启后**，重新创建 coarse_transformer 实例"
      ],
      "metadata": {
        "id": "4WAlHxvGnjHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 重启也要运行\n",
        "# import torch\n",
        "# from audiolm_pytorch import CoarseTransformer, AudioLMSoundStream\n",
        "\n",
        "# # 重新创建模型架构\n",
        "# coarse_transformer = CoarseTransformer(\n",
        "#     num_semantic_tokens = wav2vec.codebook_size,\n",
        "#     codebook_size = 1024,\n",
        "#     num_coarse_quantizers = 4,\n",
        "#     dim = 1024,\n",
        "#     depth = 6,\n",
        "#     audio_text_condition = True\n",
        "# ).cuda()\n",
        "\n",
        "# # 加载之前保存的状态字典\n",
        "# coarse_transformer.load_state_dict(torch.load('coarse_transformer.pth'))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Bs4GPmldnjHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_fine_transformer()"
      ],
      "metadata": {
        "trusted": true,
        "id": "cSJE2hqDnjHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **重启后**，重新创建 fine_transformer 实例"
      ],
      "metadata": {
        "id": "ugKBdYHjnjHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 重启也要运行\n",
        "# import torch\n",
        "# from audiolm_pytorch import CoarseTransformer, AudioLMSoundStream\n",
        "\n",
        "# # 重新创建模型架构\n",
        "# fine_transformer = FineTransformer(\n",
        "#     num_coarse_quantizers = 4,\n",
        "#     num_fine_quantizers = 8,\n",
        "#     codebook_size = 1024,\n",
        "#     dim = 1024,\n",
        "#     depth = 6,\n",
        "#     audio_text_condition = True\n",
        "# ).cuda()\n",
        "\n",
        "# # 加载之前保存的状态字典\n",
        "# fine_transformer.load_state_dict(torch.load('fine_transformer.pth'))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QTHQIBwFnjHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, CoarseTransformer, FineTransformer, AudioLM, MusicLMSoundStream\n",
        "\n",
        "# # 重新创建 wav2vec 实例\n",
        "# wav2vec = HubertWithKmeans(\n",
        "#     checkpoint_path='hubert_base_ls960.pt',\n",
        "#     kmeans_path='hubert_base_ls960_L9_km500.bin'\n",
        "# )\n",
        "\n",
        "# # 重新创建 soundstream 实例\n",
        "# soundstream = MusicLMSoundStream()\n",
        "\n",
        "# # 重新创建 SemanticTransformer 实例\n",
        "# semantic_transformer = SemanticTransformer(\n",
        "#     num_semantic_tokens=wav2vec.codebook_size,\n",
        "#     dim=1024, depth=6,\n",
        "#     audio_text_condition=True\n",
        "# ).cuda()\n",
        "\n",
        "# # 加载之前保存的 SemanticTransformer 状态\n",
        "# semantic_transformer.load_state_dict(torch.load('semantic_transformer.pth'))\n",
        "\n",
        "# # 重新创建 CoarseTransformer 实例\n",
        "# coarse_transformer = CoarseTransformer(\n",
        "#     num_semantic_tokens=wav2vec.codebook_size,\n",
        "#     codebook_size=1024,\n",
        "#     num_coarse_quantizers=4,\n",
        "#     dim=1024,\n",
        "#     depth=6,\n",
        "#     audio_text_condition=True\n",
        "# ).cuda()\n",
        "\n",
        "# # 加载之前保存的 CoarseTransformer 状态\n",
        "# coarse_transformer.load_state_dict(torch.load('coarse_transformer.pth'))\n",
        "\n",
        "# # 重新创建 FineTransformer 实例\n",
        "# fine_transformer = FineTransformer(\n",
        "#     num_coarse_quantizers=4,\n",
        "#     num_fine_quantizers=8,\n",
        "#     codebook_size=1024,\n",
        "#     dim=1024,\n",
        "#     depth=6,\n",
        "#     audio_text_condition=True\n",
        "# ).cuda()\n",
        "\n",
        "# # 加载之前保存的 FineTransformer 状态\n",
        "# fine_transformer.load_state_dict(torch.load('fine_transformer.pth'))\n",
        "\n",
        "# # 创建 AudioLM 实例\n",
        "# audiolm = AudioLM(\n",
        "#     wav2vec=wav2vec,\n",
        "#     codec=soundstream,\n",
        "#     semantic_transformer=semantic_transformer,\n",
        "#     coarse_transformer=coarse_transformer,\n",
        "#     fine_transformer=fine_transformer\n",
        "# )\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KC4f0nwPnjHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. musiclm"
      ],
      "metadata": {
        "id": "YsTSMxCenjHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you need the trained AudioLM (audio_lm) from above\n",
        "# with the MulanEmbedQuantizer (mulan_embed_quantizer)\n",
        "\n",
        "from musiclm_pytorch import MusicLM\n",
        "\n",
        "musiclm = MusicLM(\n",
        "    audio_lm = audiolm,\n",
        "    mulan_embed_quantizer = quantizer\n",
        ").cuda()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_2yetJScnjHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 1) # sample 4 and pick the top match with mulan"
      ],
      "metadata": {
        "trusted": true,
        "id": "6ETViNGVnjHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(music, 'generated_music.pt')"
      ],
      "metadata": {
        "trusted": true,
        "id": "FKp-uWHznjHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"out.wav\"\n",
        "sample_rate = 44100\n",
        "torchaudio.save(output_path, music.cpu() , sample_rate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "274TmO6gnjHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}